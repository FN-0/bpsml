"""
=============================================================
Receiver Operating Characteristic (ROC) with cross validation
=============================================================

Example of Receiver Operating Characteristic (ROC) metric to evaluate
classifier output quality using cross-validation.

ROC curves typically feature true positive rate on the Y axis, and false
positive rate on the X axis. This means that the top left corner of the plot is
the "ideal" point - a false positive rate of zero, and a true positive rate of
one. This is not very realistic, but it does mean that a larger area under the
curve (AUC) is usually better.

The "steepness" of ROC curves is also important, since it is ideal to maximize
the true positive rate while minimizing the false positive rate.

This example shows the ROC response of different datasets, created from K-fold
cross-validation. Taking all of these curves, it is possible to calculate the
mean area under curve, and see the variance of the curve when the
training set is split into different subsets. This roughly shows how the
classifier output is affected by changes in the training data, and how
different the splits generated by K-fold cross-validation are from one another.

.. note::

    See also :func:`sklearn.metrics.roc_auc_score`,
             :func:`sklearn.model_selection.cross_val_score`,
             :ref:`sphx_glr_auto_examples_model_selection_plot_roc.py`,

"""
#print(__doc__)

import sys
import pickle
import numpy as np
import pandas as pd
from scipy import interp
import matplotlib.pyplot as plt

from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc, accuracy_score
from sklearn.model_selection import StratifiedKFold, ShuffleSplit, train_test_split

# #############################################################################
# Data IO and generation

# Import some data to play with
#df = pd.read_csv('.\\ready2train\\trainning_data.csv', header=0)
df = pd.read_csv(sys.argv[1], header=0)
#X = df.iloc[0:184, 1:].to_numpy()
X = df.iloc[0:, 1:].to_numpy()
#y = df.iloc[0:184, 0].to_numpy()
y = df.iloc[0:, 0].to_numpy()
#X, y = X[y != 2], y[y != 2]
n_samples, n_features = X.shape

# Add noisy features
random_state = np.random.RandomState(0)
#X = np.c_[X, random_state.randn(n_samples, 40 * n_features)]

# #############################################################################
# Classification and ROC analysis

# Run classifier with cross-validation and plot ROC curves
#cv = StratifiedKFold(n_splits=6)
#cv = ShuffleSplit(n_splits=3, test_size=0.3)
classifier = svm.SVC(kernel='linear', probability=True,
                     random_state=random_state)

tprs = []
aucs = []
accs = []
mean_fpr = np.linspace(0, 1, 100)

plt.figure(figsize=(4.65, 4.1))
plt.plot(1, 1, lw=0, alpha=.8, label='Genes:   84       n      Acc.     AUC')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.7)#,
                                                    #random_state=100)
f = open('save/svc_20200217_150648.pickle', 'rb')
clf = pickle.load(f)
#fit(X_train, y_train)

# #############################################################################
# Training
acc = accuracy_score(y_train, clf.predict(X_train))
probas_ = clf.predict_proba(X_train)
# Compute ROC curve and area the curve
y_train_bin = []
for y in y_train:
    if y == 'cancer':
        y_train_bin.append(0)
    else:
        y_train_bin.append(1)
fpr, tpr, thresholds = roc_curve(y_train_bin, probas_[:, 1])
tprs.append(interp(mean_fpr, fpr, tpr))
tprs[-1][0] = 0.0
roc_auc = auc(fpr, tpr)
aucs.append(roc_auc)
accs.append(acc)
plt.plot(fpr*100, tpr*100, lw=2, alpha=.7, linestyle='-', color='grey',
          label='         Training   104  %d%%   %0.3f' % (acc*100, roc_auc))
          #label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))

# #############################################################################
# Evaluation
acc = accuracy_score(y_test, clf.predict(X_test))
probas_ = clf.predict_proba(X_test)
# Compute ROC curve and area the curve
y_test_bin = []
for y in y_test:
    if y == 'cancer':
        y_test_bin.append(0)
    else:
        y_test_bin.append(1)
fpr, tpr, thresholds = roc_curve(y_test_bin, probas_[:, 1])
tprs.append(interp(mean_fpr, fpr, tpr))
tprs[-1][0] = 0.0
roc_auc = auc(fpr, tpr)
aucs.append(roc_auc)
accs.append(acc)
plt.plot(fpr*100, tpr*100, lw=2, alpha=.8,
          label='     Evaluation   69    %d%%    %0.3f' % (acc*100, roc_auc))
          #label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))

# #############################################################################
# Validation
'''mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
plt.plot(mean_fpr*100, mean_tpr*100, color='b',
         label='      Validation   173  %d%%    %0.2f' % (sum(accs)/len(accs)*100, mean_auc),
         lw=2, alpha=.8)'''

plt.xlim([-5, 105])
plt.ylim([-5, 105])
plt.xlabel('100% - Specificity (%)')
plt.ylabel('Sensitivity (%)')
#plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.show()
